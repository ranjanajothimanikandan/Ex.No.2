
# Ex.No: 2 	Evaluation of 2024 Prompting Tools Across Diverse AI Platforms: ChatGPT, Claude, Bard, Cohere Command, and Meta 
### DATE:                                                                            
### REGISTER NUMBER : 
 
### Aim:
To compare the performance, user experience, and response quality of different AI platforms (ChatGPT, Claude, Bard, Cohere Command, and Meta) within a specific use case, such as summarizing text or answering technical questions. Generate a Prompt based output using different Prompting tools of 2024.
### AI Tools required:

### Explanation:
Define the Use Case:
Select a specific task for evaluation across platforms (e.g., summarizing a document, answering a technical question, or generating a creative story / Code).
Ensure the use case is applicable to all platforms and will allow for comparison across response quality, accuracy, and depth.
Create a Set of Prompts:
Prepare a uniform set of prompts that align with the chosen use case.
Each prompt should be clear and precise, ensuring that all platforms are evaluated using the same input.
Consider multiple prompts to capture the versatility of each platform in handling different aspects of the use case.
Run the Experiment on Each AI Platform:
Input the prompts into each AI tool (ChatGPT, Claude, Bard, Cohere Command, and Meta) and gather the responses.
Ensure the same conditions are applied for each platform, such as input format, time to respond, and prompt delivery.
Record response times, ease of interaction with the platform, and any technical issues encountered.
Evaluate Response Quality:
Assess each platform’s responses using the following criteria: Accuracy,Clarity,Depth,Relevance 
Compare Performance:
Compare the collected data to identify differences in performance across platforms.
Identify any platform-specific advantages, such as faster response times, more accurate answers, or more intuitive interfaces.
Deliverables:
A comparison table outlining the performance of each platform (ChatGPT, Claude, Bard, Cohere Command, and Meta) based on accuracy, clarity, depth, and relevance of responses.
A final report summarizing the findings of the experiment, including recommendations on the most suitable AI platform for different use cases based on performance and user 

## AI TOOLS :
To evaluate the effectiveness of prompting tools across multiple AI
platforms—namely ChatGPT (OpenAI), Claude (Anthropic), Bard (Google),
Cohere Command, and Meta's AI models—focusing on diverse NLP use
cases such as summarization, technical Q&A, and creative content
generation.
## Overview of Prompting in AI :
Prompting tools have become pivotal in NLP applications, acting as the
interface through which users communicate complex tasks to AI systems.
These tools have evolved to support refined interaction, improved output
control, and diverse contextual understanding.
## Purpose of This Evaluation:

This study aims to assess and compare the capabilities, user experiences,
and output quality of several leading AI platforms. The comparison targets
tasks such as text summarization, code-related queries, and creative writing
to identify each tool's strengths and areas for growth.
## Evaluation Methodology
Use Case Selection:
Defined use cases:

Summarizing technical or legal content
Answering detailed technical/coding questions
Generating creative or narrative text
Comparison Criteria:
Response Quality: Accuracy, relevance, fluency, clarity
Performance: Response speed, reliability, and prompt consistency
User Experience: Interaction simplicity, customization, and control
Data Privacy: Handling of sensitive information and transparency
Prompt Complexity Handling: From simple queries to context-rich
instructions
Customization & Fine-Tuning: Flexibility through prompt design or settings
like temperature, system instructions
## Use Case 1: Summarizing Complex Text
Prompt Setup:
Each platform was tested using a dense, domain-specific document (e.g.,
legal policy or scientific article) and asked to produce summaries of varying
lengths and styles.
## Platform-Specific Observations:
ChatGPT: Offers structured summaries and strong coherence, but may
simplify complex jargon.
Claude: Notable for context retention and brevity, with clear paragraphing.
Bard: Context-rich summaries, often integrating external data, though

sometimes verbose.
## Comparative Insights:
Readability & Precision: Claude and ChatGPT lead in clarity; Bard shines in
information depth.
Terminology Handling: Claude showed stronger retention of domain-
specific language.
Summary Length Control & Rephrasing: ChatGPT and Claude handle these
well; Bard less so.
Use Case 2: Answering Technical Questions
## Prompt Setup:
A set of prompts required platforms to write or debug code, explain
algorithms, or troubleshoot technical errors across multiple programming
languages.
## Platform-Specific Observations:
ChatGPT: Excellent coding support, logical explanations, and debug
assistance.
Claude: Superior at reasoning over multiple steps; maintains long-context
queries well.
Bard: Good with basic queries but occasionally outdated or imprecise.
## Comparative Insights
Code Accuracy & Reasoning: ChatGPT and Claude perform best.
Error Handling: Claude detects and explains bugs more reliably.
Language Support: ChatGPT handles language transitions smoothly.

Latest Tech Knowledge: ChatGPT stays most current; Bard is occasionally
behind.
5. Use Case 3: Text Generation & Creative Tasks
## Prompt Setup:
Used for storytelling, rewriting, brainstorming, and generating content with
emotional or humorous tones.
## Platform-Specific Observations:
ChatGPT: Balances creativity and logic; highly adaptable in tone.
Claude: Strong storytelling flow with thoughtful development.
Bard: Generates unique ideas but less refined narrative structure.
## Comparative Insights
Originality & Engagement: Claude is more imaginative; ChatGPT is more
structured.
Tone/Genre Flexibility: ChatGPT leads with seamless transitions.
Handling Sensitive Content: Claude is cautious but balanced; ChatGPT flags
content appropriately.
## Performance and Technical Metrics
Speed: Bard is typically fastest; ChatGPT is consistent but slower with long
prompts.
Context Retention: Claude excels in multi-turn, long-context chats.
Scalability & Load: All platforms handle load well, though Bard shows slight
delays under strain.
5
Error Handling: ChatGPT recovers gracefully from incomplete inputs;
Claude prompts for clarification.
## User Experience Overview
Ease of Use: ChatGPT and Bard offer the most intuitive interfaces.
Control Mechanisms: ChatGPT allows detailed prompt tuning (e.g., system
roles, temperature).
User Adaptability: Claude adjusts mid-conversation better than others.
Beginner Friendliness: Bard and ChatGPT are easiest for new users.
Enterprise Features: ChatGPT provides enterprise APIs and team tools.
Feedback Integration: ChatGPT and Claude actively incorporate user
feedback to improve responses.
## Conclusion & Recommendations
Summary of Key Findings:
Best for Summarization: Claude, followed by ChatGPT.
Best for Technical Tasks: ChatGPT, then Claude.
Best for Creative Work: Claude leads in creativity; ChatGPT in structure and
coherence.

## Industry-Specific Suggestions
Healthcare & Legal: Claude (better jargon handling)
Software & Engineering: ChatGPT (technical accuracy)
Marketing & Media: Claude or Bard (for creative ideation)
## Limitations & Workarounds
Bard may lack depth; pairing with ChatGPT enhances detail.
Claude struggles with newer APIs; ChatGPT compensates.
## Cost Consideration
⦁ ChatGPT and Claude offer freemium and enterprise models.
⦁ Bard currently remains free but lacks robust customization.
⦁ Value depends on usage intensity and precision required.
## Future Outlook
⦁ Long-term memory
⦁ Multi-modal prompting (voice/image)
⦁ Better feedback loops
⦁ Real-time conversational agents


## Result:
Thus, the evaluation of 2024 prompting tools across leading AI platforms-
ChatGpt , Claude , Bard ,Cohere Command, and Meta’s based models has
been analysed.
